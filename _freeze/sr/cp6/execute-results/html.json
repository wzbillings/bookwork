{
  "hash": "e65a35a3e4960831c15d8d6a1b85f772",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 6: The Haunted DAG and the Causal Terror\"\ndate: 2023-05-17\n---\n\n\n\n\n\nThis chapter discusses three common pitfalls that can lead our statistical models to misbehave and make our causal interpretations difficult or incorrect. The three major topics are collider bias (selection-distortion), multicollinearity in regression models, and post-treatment bias. The chapter further expands on the idea of DAGs as graphical causal models that was introduced in the previous chapter.\n\n## Chapter notes\n\n* The **Selection-distortion effect** (AKA Berkson's/Berksonian bias, generalized to the idea of collider bias) occurs when the selection of a sample changes the relationship between the observed variables. (I.e. there is/isn't a relationship between the two variables on the sample, but in the larger population, there isn't/is a relationship.) Berkson's bias refers to the particular effect that when selecting from a population on two desirable traits, there often appears to be a negative correlation between the desirable traits in the selected sample.\n* **Multicollinearity** refers to a very strong association between two or more predictor variables, conditional on the other variables in the model. When variables are multicollinear, the posterior distribution will seem to suggest that none of the multicollinear variables are truly associated with the outcome, even if the reality is that they are all strongly associated.\n* **Post-treatment bias**: a form of *included variable bias* where variables\nthat are also causal descendents of the treatment, are controlled for when\nassessing the response. That is, you measure something that is not the\noutcome of interest and is also affected by the treatment, and you adjust for\nthat quantity when analyzing the outcome. This induces collider bias.\n* Controlling for a collider on a DAG induces *D*-separation, meaning that\nthe DAG is no longer connected.\n* When you condition on a collider (a common descendent), it creates statistical,\nalthough not necessarily causal, relationships between the ancestors.\n* Even unmeasured causes can induce collider bias. Selection bias in a study\ncan often be interpreted as conditioning on a collider during the sampling\nprocess. See the parents and grandparents example in section 6.3.2.\n* There are four types of elemental confounds: DAG structures that allow\nus to determine causal and non-causal pathways.\n\t+ The fork: two variables have a common cause (Z -> X; Z -> Y).\n\t+ The pipe: one variable is intermediate in the causal relationship between\n\ttwo others (X -> Z -> Y).\n\t+ The collider: two variables have a common descenent (X -> Z; Y -> Z).\n\t+ The descendant: a variable which descends from another, capturing part\n\tof the ancestor's statistical signal (in the previous example, if we also\n\thave Z -> D, D will appear to be a collider as well, even if it is just\n\ta descendant).\n* Every DAG is built out of these four types of relationships, and we can\nuse specific rules for DAGs to determine what variables need to be included\nin models for a causal effect.\n* \"Multiple regression is no oracle, but only a golem.\"\n\n## Exercises\n\n### 6E1\n\nThree mechanisms that can produce false inferences about causal\neffects in a multiple regression model are: multicollinearity, the selection-\ndistortion effect, and post-treatment bias.\n\n### 6E2\n\nFor an example of post-treatment bias, consider a vaccine efficacy trial for\ninfluenza (or I guess any disease). Suppose we have a known surrogate of\nprotection, an immunological measurement that is strongly associated with\nprotection from the disease. For influenza, one potential biomarker is\nhemagglutinin inhibition (HI) titer, which is typically measured before\nand after (around 21 to 28 days) vaccination. If one did a challenge study or\na long followup period of surveillance, we can record which individuals are\nultimately infected with influenza. Including participants' HI titers *before*\nvaccination when modeling vaccine protection is OK depending on the context,\nbut including participants HI titer *after*\nvaccination would induce post-treatment bias, because vaccination directly\naffects HI titer.\n\n### 6E3\n\n1. The fork (`Z -> X`; `Z -> Y`): `X тлл Y | Z`\n1. The pipe (`X -> Z -> Y`): `X тлл Y | Z`\n1. The collider (`X -> Z`; `Y -> Z`): `X тлл Y`; `X !тлл Y | Z`\n1. The descendent: conditional independencies are the same as the parent\n\n### 6E4\n\nSuppose we have two variables (call one the exposure and the other the outcome)\nwhich are both causes of a third variable. If that third variable determines\nwhich observations we observe (for example, a restaurant existing or a\npatient agreeing to participate in a study), in our observed sample\nwe will see a correlation between the exposure and the outcome, just because\nwe are only seeing observations where the third variable is already specified.\n\nIn the funded grants example, a funded grant must be high in at least one of\nnewsworthiness or trustworthiness, otherwise it will not be funded. If we\ncould see all grants, we would not see a correlation between newsworthiness\nand trustworthiness. But when we only look at funded grants, we condition on\na common descendant of both variables (a collider), which makes a spurious\nrelationship appear.\n\n## 6M1\n\nThe new DAG including $V$ as an unobserved common cause of $C$ and $Y$ looks\nlike this.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndag_6m1 <- dagitty::dagitty(\n\t\"dag {\n\t\tU [unobserved]\n\t\tV [unobserved]\n\t\tX -> Y\n\t\tX <- U <- A -> C -> Y\n\t\tU -> B <- C\n\t\tC <- V -> Y\n\t}\"\n)\ndagitty::coordinates(dag_6m1) <-\n\tlist(\n\t\tx = c(U = 1, V = 4, X = 1, Y = 3, A = 2, B = 2, C = 3),\n\t\ty = c(U = 1.5, V = 2.5, X = 3, Y = 3, A = 1, B = 2, C = 1.5)\n\t)\ndagitty::exposures(dag_6m1) <- \"X\"\ndagitty::outcomes(dag_6m1) <- \"Y\"\nplot(dag_6m1)\n```\n\n::: {.cell-output-display}\n![](cp6_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\nWe still have all of the same paths from the previous example, i.e.:\n\n* $X \\to Y$,\n* $X \\leftarrow U \\leftarrow A \\to C \\to Y$, and\n* $X \\leftarrow U \\to B \\leftarrow C \\to Y$.\n\nThe first path is the direct path. The second is an open backdoor path through\n$A$. The third is a closed backdoor path, as it passes through the collider $B$.\nBy adding the unobserved confounder $V$, we create two new backdoor paths,\n\n* $X \\leftarrow U \\leftarrow A \\to C \\leftarrow V \\to Y$, and\n* $X \\leftarrow U \\to B \\leftarrow C \\leftarrow V \\to Y$.\n\nThe first path is an open backdoor path, and the second path is a closed\nbackdoor path. We can check which paths exist and are open with dagitty.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndagitty::paths(dag_6m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$paths\n[1] \"X -> Y\"                     \"X <- U -> B <- C -> Y\"     \n[3] \"X <- U -> B <- C <- V -> Y\" \"X <- U <- A -> C -> Y\"     \n[5] \"X <- U <- A -> C <- V -> Y\"\n\n$open\n[1]  TRUE FALSE FALSE  TRUE FALSE\n```\n\n\n:::\n:::\n\n\n\nNow we need to close the two open paths, without opening either of the closed\npaths. Conditioning on $C$ would close both of the open paths, but would also\nopen the fifth path. However, conditioning on $A$ will close both open\npaths without opening either of the closed paths. So $\\{A\\}$ is our\nsufficient adjustment set. We can verify this with dagitty.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndagitty::adjustmentSets(dag_6m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n{ A }\n```\n\n\n:::\n:::\n\n\n\n## 6M2\n\nFirst we'll do the simulation: we want $X$ and $Z$ to be highly correlated.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(101)\nX <- rnorm(100, 0, 1)\nZ <- rnorm(100, X, 0.5)\nY <- rnorm(100, Z, 1)\n\ncor(cbind(X, Z, Y))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          X         Z         Y\nX 1.0000000 0.8925919 0.6779960\nZ 0.8925919 1.0000000 0.7998096\nY 0.6779960 0.7998096 1.0000000\n```\n\n\n:::\n:::\n\n\n\nWe can see that all of the variables are strongly associated, but $X$ and $Z$\nhave a particularly strong correlation.\nBut now we want to use a model that adjusts for both.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_6m1 <-\n\trethinking::quap(\n\t\tflist = alist(\n\t\t\tY ~ dnorm(mu, sigma),\n\t\t\tmu <- a + bX * X + bZ * Z,\n\t\t\ta ~ dnorm(0, 0.5),\n\t\t\tc(bX, bZ) ~ dnorm(0, 1),\n\t\t\tsigma ~ dexp(1)\n\t\t),\n\t\tdata = list(X = X, Y = Y, Z = Z)\n\t)\n\ncoeftab(fit_6m1) |>\n\tcoeftab_plot(pars = c(\"bX\", \"bZ\"))\n```\n\n::: {.cell-output-display}\n![](cp6_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\nInterestingly, we can see that we do not get the same problem as the previous\nmulticollinearity example. The confidence intervals appear to be reasonable,\nand we see a strong effect of $Z$ but no effect of $Y$. Intuitively, this make\nsense -- $Z$ and $Y$ have a stronger correlation than $X$ and $Y$, so after\nwe control for $Z$, the model \"finds\" all of the signal, and then does not\nfind an effect of $X$. So if we interpreted this model without considering the\ncausal framework, we would still be mislead by the multicollinearity, but\nthere is nothing obviously wrong -- the entire causal effect of $X$ on $Z$ is\nthrough $Z$, so this estimate of the direct causal effect of $X$ makes sense.\n\n## 6M3\n\nHere are the adjustment sets for each of the DAGs shown.\n\n* Top left DAG: `Z` only\n* Top right: nothing\n* Bottom left: nothing\n* Bottom right: `A` only\n\nI also checked using dagitty to verify my answers.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndag1 <- dagitty::dagitty(\"dag {Z -> X -> Y; A -> Z -> Y; A -> Y}\")\ndag2 <- dagitty::dagitty(\"dag {X -> Z -> Y; A -> Z -> Y; A -> Y}\")\ndag3 <- dagitty::dagitty(\"dag {X -> Y -> Z; A -> X -> Z; A -> Z}\")\ndag4 <- dagitty::dagitty(\"dag {A -> X -> Z; A -> Z -> Y; X -> Y}\")\n\nlapply(list(dag1, dag2, dag3, dag4), dagitty::adjustmentSets, \"X\", \"Y\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n{ Z }\n\n[[2]]\n {}\n\n[[3]]\n {}\n\n[[4]]\n{ A }\n```\n\n\n:::\n:::\n\n\n\n## 6H1\n\nUsing the Waffle House data, we want to find the total causal influence of\nnumber of Waffle Houses on divorce rate. First, let's look at what we have\nto work with.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"WaffleDivorce\")\nhead(WaffleDivorce)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    Location Loc Population MedianAgeMarriage Marriage Marriage.SE Divorce\n1    Alabama  AL       4.78              25.3     20.2        1.27    12.7\n2     Alaska  AK       0.71              25.2     26.0        2.93    12.5\n3    Arizona  AZ       6.33              25.8     20.3        0.98    10.8\n4   Arkansas  AR       2.92              24.3     26.4        1.70    13.5\n5 California  CA      37.25              26.8     19.1        0.39     8.0\n6   Colorado  CO       5.03              25.7     23.5        1.24    11.6\n  Divorce.SE WaffleHouses South Slaves1860 Population1860 PropSlaves1860\n1       0.79          128     1     435080         964201           0.45\n2       2.05            0     0          0              0           0.00\n3       0.74           18     0          0              0           0.00\n4       1.22           41     1     111115         435450           0.26\n5       0.24            0     0          0         379994           0.00\n6       0.94           11     0          0          34277           0.00\n```\n\n\n:::\n:::\n\n\n\nOK, so of course we will assume that there is a direct causal effect of number\nof Waffle Houses ($W$) on divorce rate ($D$). From previous work, we know\nour data are consistent with a $M \\to A \\to D$ DAG structure, for $M$ the\nmarriage rate and $A$ the median age at marriage, so we'll incorporate this\ninto our DAG. We also saw that the data are consistent with being in the\nSouth affecting $M$ and $A$, so we'll include that in our DAG, and of course\nwe expect to see $S \\to W$. Finally, since we're trying to find the **total**\ncausal effect of $W$, we'll include $A \\leftarrow W \\rightarrow M$ as a sub-DAG\nas well. Putting it all together, our DAG looks like this.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndag_6h1 <-\n\tdagitty::dagitty(\n\t\t\"dag {\n\t\tM -> A -> D\n\t\tM <- S -> A\n\t\tS -> W\n\t\tW -> D\n\t\tM <- W -> A\n\t\t}\"\n\t)\ndagitty::exposures(dag_6h1) <- \"W\"\ndagitty::outcomes(dag_6h1) <- \"D\"\ndagitty::coordinates(dag_6h1) <-\n\t\tlist(\n\t\tx = c(S = 1, W = 2, D = 2.7, M = 1, A = 2),\n\t\ty = c(S = 1, W = 1, D = 1.5, M = 2, A = 2)\n\t)\n\nplot(dag_6h1)\n```\n\n::: {.cell-output-display}\n![](cp6_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\nLet's now figure out what needs to be in our model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndagitty::adjustmentSets(dag_6h1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n{ S }\n```\n\n\n:::\n:::\n\n\n\nWe see that to get the total cause effect of $W$ on $D$, we need only\nadjust for $S$, being in the South. I should probably worry about things like\ntransformations and zero-inflation, but for this exercise I am not going to do\nthat.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwd <- \n\tlist(\n\t\tW = standardize(WaffleDivorce$WaffleHouses),\n\t\tD = standardize(WaffleDivorce$Divorce),\n\t\tS = WaffleDivorce$South + 1\n\t)\n```\n:::\n\n\n\nNow we'll fit the model. I'll allow the intercept to be different for Southern\nand non-Southern states, but because we're interested in the total causal effect\nof Waffle House Numbers, I'll force the effect to be the same across both groups.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(100)\nfit_6h1 <-\n\trethinking::quap(\n\t\tflist = alist(\n\t\t\tD ~ dnorm(mu, sigma),\n\t\t\tmu <- a[S] + b * W,\n\t\t\ta[S] ~ dnorm(0, 1),\n\t\t\tb ~ dnorm(0, 2),\n\t\t\tsigma ~ dexp(1)\n\t\t),\n\t\tdata = wd\n\t)\n\nrethinking::precis(fit_6h1, depth = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             mean         sd        5.5%     94.5%\na[1]  -0.18668305 0.16832424 -0.45569771 0.0823316\na[2]   0.46329565 0.29995319 -0.01608748 0.9426788\nb      0.05197162 0.17641807 -0.22997853 0.3339218\nsigma  0.92058385 0.09085178  0.77538516 1.0657825\n```\n\n\n:::\n:::\n\n\n\nOK, I'm getting a warning but I don't think it's doing anything. So I'll just\nignore it. For this model, we can see that there is a **strongly positive**\neffect of $b$. Let's look at the posterior distribution.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npost_6h1 <- extract.samples(fit_6h1)\ndens(post_6h1$b)\nabline(v = 0, lty = 2)\n```\n\n::: {.cell-output-display}\n![](cp6_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\nWe can see that almost all of the posterior density is above zero, indicating\nthat there is a **positive effect** of Waffle Houses on divorce rate. For every\n1 standard deviation increase in the number of Waffle Houses in a state, the\ndivorce rate is expected to increase by about 0.25 units. Let's put that back\nfrom standardized units into real units.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 * attr(wd$W, \"scaled:scale\") + attr(wd$W, \"scaled:center\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 98.12959\n```\n\n\n:::\n\n```{.r .cell-code}\n0.25 * attr(wd$D, \"scaled:scale\") + attr(wd$D, \"scaled:center\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 10.1432\n```\n\n\n:::\n:::\n\n\n\nSo we see that we expect the divorce rate to increase by about $10\\%$ for every\n98 additional Waffle Houses, or approximately $0.1\\%$ per Waffle House. This\nis the total causal effect based on our DAG, even though I would guess that the\ndirect effect is zero and this entire effect is through location.\n\n### 6H2\n\nFirst, we need to check the implied causal independencies of the DAG.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndagitty::impliedConditionalIndependencies(dag_6h1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nD _||_ M | A, W\nD _||_ S | A, W\n```\n\n\n:::\n:::\n\n\n\nTest one: $D$ and $M$ should be independent after adjusting for $A$ and\n$W$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(100)\nwd2 <- c(\n\twd,\n\tM = list(standardize(WaffleDivorce$Marriage)),\n\tA = list(standardize(WaffleDivorce$MedianAgeMarriage))\n)\n\nfit_6h2_a1 <-\n\trethinking::quap(\n\t\tflist = alist(\n\t\t\tD ~ dnorm(mu, sigma),\n\t\t\tmu <- a + bm * M + ba * A + bw * W,\n\t\t\ta ~ dnorm(0, 1),\n\t\t\tc(bm, ba, bw) ~ dnorm(0, 2),\n\t\t\tsigma ~ dexp(1)\n\t\t),\n\t\tdata = wd2\n\t)\n\nfit_6h1_a2 <-\n\t\trethinking::quap(\n\t\tflist = alist(\n\t\t\tD ~ dnorm(mu, sigma),\n\t\t\tmu <- a + bm * M,\n\t\t\ta ~ dnorm(0, 1),\n\t\t\tc(bm) ~ dnorm(0, 2),\n\t\t\tsigma ~ dexp(1)\n\t\t),\n\t\tdata = wd2\n\t)\n\nrethinking::coeftab(\n\tfit_6h2_a1,\n\tfit_6h1_a2\n) |>\n\trethinking::coeftab_plot(pars = c(\"bm\"))\n```\n\n::: {.cell-output-display}\n![](cp6_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\nYes, we can see that if we only include $M$ (`fit_6h1_a2`), we see an effect,\nbut if we control for $A$ and $W$, as in `fit_6h1_a1`, we do not.\n\nNow let's check the second test: $D$ and $S$ should be independent if we\ncontrol for $A$ and $W$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(100)\nfit_6h2_b1 <-\n\trethinking::quap(\n\t\tflist = alist(\n\t\t\tD ~ dnorm(mu, sigma),\n\t\t\tmu <- a + bs * S + ba * A + bw * W,\n\t\t\ta ~ dnorm(0, 1),\n\t\t\tc(bs, ba, bw) ~ dnorm(0, 2),\n\t\t\tsigma ~ dexp(1)\n\t\t),\n\t\tdata = wd2\n\t)\n\nfit_6h1_b2 <-\n\t\trethinking::quap(\n\t\tflist = alist(\n\t\t\tD ~ dnorm(mu, sigma),\n\t\t\tmu <- a + bs * S,\n\t\t\ta ~ dnorm(0, 1),\n\t\t\tc(bs) ~ dnorm(0, 2),\n\t\t\tsigma ~ dexp(1)\n\t\t),\n\t\tdata = wd2\n\t)\n\nrethinking::coeftab(\n\tfit_6h2_b1,\n\tfit_6h1_b2\n) |>\n\trethinking::coeftab_plot(pars = c(\"bs\"))\n```\n\n::: {.cell-output-display}\n![](cp6_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\nWe see exactly the same interpretation here: when only $S$ is in the model,\nall of the posterior density is above 0, but when we control for $A$ and $W$,\na significant portion is below zero and the mean is lower. So I think we\ncan say that our data appear to be consistent with the conditional\nindependencies that our DAG implies.\n\nWe can also do these tests automatically used the method recommended by dagitty.\nI don't know that much about these results, but they agree with our modeling\nresults, which is good!\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndagitty::localTests(\n\tdag_6h1,\n\tsample.cov = lavaan::lavCor(as.data.frame(wd2)),\n\tsample.nobs = nrow(as.data.frame(wd2))\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  estimate   p.value       2.5%     97.5%\nD _||_ M | A, W -0.0855530 0.5650794 -0.3613429 0.2035528\nD _||_ S | A, W  0.1349902 0.3622409 -0.1550991 0.4044034\n```\n\n\n:::\n:::\n\n\n\n## 6H3\n\nNow we are back to the fox problems. I'll reproduce the DAG first just so I\nhave it in my notes.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfox_dag <-\n\tdagitty::dagitty(\n\t\t\"dag {\n\t\tA -> F -> G -> W\n\t\tF -> W\n\t\t}\"\n\t)\ndagitty::coordinates(fox_dag) <-\n\tlist(\n\t\tx = c(A = 2, F = 1, G = 3, W = 2),\n\t\ty = c(A = 1, F = 2, G = 2, W = 3)\n\t)\nplot(fox_dag)\n```\n\n::: {.cell-output-display}\n![](cp6_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\nFirst, we want to infer the total causal influence of area (A) on weight (W).\nI'll go ahead and set up the data, and standardize all the variables as\nMcElreath recommends. I'll also model the log weight instead of the raw weight\nso we can ensure that our predictions remain positive.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(foxes)\nf2 <-\n\tfoxes |>\n\tdplyr::transmute(\n\t\tA = area,\n\t\tF = avgfood,\n\t\tG = groupsize,\n\t\tW = log(weight)\n\t) |>\n\tas.list() |>\n\tlapply(FUN = rethinking::standardize)\n```\n:::\n\n\n\nNow we'll adopt a model for the weight. Since it is logged and standardized\nwe'll use a Gaussian likelihood function. To set priors, we first need to\ndetermine our adjustment set. There are two causal paths between $A$ and $W$:\n\n1. $A \\to F \\to W$ and\n2. $A \\to F \\to G \\to W$.\n\nIf we want the total causal effect of $A$ on $W$, there are no closed paths\nand we do not need to adjust for any other variables. We can confirm this\nwith `dagitty`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndagitty::adjustmentSets(\n\tfox_dag,\n\texposure = \"A\",\n\toutcome = \"W\",\n\teffect = \"total\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n {}\n```\n\n\n:::\n:::\n\n\n\nWe get the empty set as our minimal sufficient adjustment set. So we'll adopt\nthe following model.\n\n$$\n\\begin{align*}\n\\mathrm{Scale}\\left(\\log W \\right) &\\sim N(\\mu, \\sigma) \\\\\n\\mu &= \\alpha + \\beta \\cdot \\mathrm{Scale}(A)\\\\\n\\alpha &\\sim N(4.5, 0.5) \\\\\n\\beta &\\sim N(0, 0.25) \\\\\n\\sigma &\\sim \\mathrm{Exp}(10)\n\\end{align*}\n$$\n\nI tuned the parameters for the prior distributions using a prior predictive\nsimulation to ensure that the prior predictions stay within the expected outcome\nspace. As usual, I think that making this model have an intercept of zero\npotentially makes more sense (a fox with no area should starve) and then\nwe would not expect this relationship to be linear, but we will just mess with\nthe priors until the predictions look reasonable.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(101)\na <- rnorm(1000, 4.5, 0.5)\nb <- rnorm(1000, 0, 0.25)\nsigma <- rexp(1000, 10)\n\nplot(\n\tNULL,\n\txlim = c(1, 5.25),\n\tylim = c(1.5, 7.5),\n\txlab = \"Area\",\n\tylab = \"Weight\",\n\tmain = \"Prior predictive simulation\"\n)\n\nx <- seq(min(f2$A), max(f2$A), length.out = 1000)\nxp <- x * attr(f2$A, \"scaled:scale\") + attr(f2$A, \"scaled:center\")\nout <- vector(length = 1000, mode = \"list\")\nfor (i in 1:1000) {\n\t# Sample y's from their distribution\n\ty <- rnorm(1000, a[i] + b[i] * x, sigma[i])\n\t\n\t# Backtransform to original scale\n\typ <- exp(y * attr(f2$W, \"scaled:scale\") + attr(f2$W, \"scaled:center\"))\n\tout[[i]] <- yp\n\t\n\tlines(x = xp, y = y, type = \"l\", col = rethinking::col.alpha(\"black\", 0.05))\n}\n```\n\n::: {.cell-output-display}\n![](cp6_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n\nI think that looks fine but I have to admit that I find assigning reasonable\npriors to standardized data quite difficult, it feels like just randomly\npicked numbers under the lines look ok. Now we can finally fit our\nregression and get the estimated causal effect.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_6h3 <- rethinking::quap(\n\tflist = alist(\n\t\tW ~ dnorm(mu, sigma),\n\t\tmu <- a + b * A,\n\t\ta ~ dnorm(4.5, 0.5),\n\t\tb ~ dnorm(0, 0.25),\n\t\tsigma ~ dexp(10)\n\t),\n\tdata = f2\n)\nrethinking::precis(m_6h3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            mean         sd         5.5%     94.5%\na     0.14011603 0.08988426 -0.003536369 0.2837684\nb     0.03384817 0.08470967 -0.101534245 0.1692306\nsigma 0.96550793 0.06089243  0.868190063 1.0628258\n```\n\n\n:::\n:::\n\n\n\nLet's look at the posterior distribution of $\\beta$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npost <- extract.samples(m_6h3)\ndens(post$b, lwd = 2)\nabline(v = 0, lty = 2)\n```\n\n::: {.cell-output-display}\n![](cp6_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n\nWe can see that while the density of $\\beta$ is more than 50% above zero, there\nis a substantial amount of the density on either side of zero. So in general,\nit seems that area size is not directly correlated to fox weight. If anything,\nthere is a small positive effect, but it is not very strong. We can\nget predictions on the original scale as well.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_out <- rethinking::sim(m_6h3, data = list(A = x), n = 10000)\nmodel_mu <-\n\tcolMeans(model_out) |>\n\t(\\(x) x *attr(f2$W, \"scaled:scale\") + attr(f2$W, \"scaled:center\"))() |>\n\texp()\n\nmodel_pi <- apply(model_out, 2, \\(x) exp(rethinking::PI(x) * \n\t\t\t\t\t\t\t\t\t\tattr(f2$W, \"scaled:scale\") + attr(f2$W, \"scaled:center\")))\n\nplot(\n\txp, model_mu,\n\ttype = \"l\",\n\txlim = range(xp),\n\tylim = range(foxes$weight),\n\txlab = \"Simulated area\",\n\tylab = \"Counterfactual weight\"\n)\nlines(xp, model_pi[1, ], lty = 2)\nlines(xp, model_pi[2, ], lty = 2)\n```\n\n::: {.cell-output-display}\n![](cp6_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\n\nAs we would expect from the model estimates, there is a slight positive trend\nwith incredibly wide credible intervals.\n\n## 6H4\n\nNext we want to infer the total causal effect of adding food. Since we want\nto know the *total* causal effect, we don't need to adjust for anything\nelse in the model, since $G$ is a mediator on the causal path. This time I'll\njust use standard priors since it doesn't really matter that much.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_6h4 <- rethinking::quap(\n\tflist = alist(\n\t\tW ~ dnorm(mu, sigma),\n\t\tmu <- a + b * `F`,\n\t\ta ~ dnorm(0, 1),\n\t\tb ~ dnorm(0, 1),\n\t\tsigma ~ dexp(1)\n\t),\n\tdata = f2\n)\nrethinking::precis(m_6h4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               mean         sd       5.5%     94.5%\na      9.224085e-10 0.09165572 -0.1464835 0.1464835\nb     -1.533812e-02 0.09205000 -0.1624518 0.1317756\nsigma  9.913351e-01 0.06467093  0.8879784 1.0946917\n```\n\n\n:::\n:::\n\n\n\nOK, we see a similar thing here. There is a possibly a slight negative\nrelationship, but it looks like there is really no relationship here.\n\n## 6H5\n\nNow we want to get the total causal effect of group size, $G$. Now we have\nto also control for food, $F$, because it is a confounder on one of the\npaths from $G \\to W$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_6h5 <- rethinking::quap(\n\tflist = alist(\n\t\tW ~ dnorm(mu, sigma),\n\t\tmu <- a + bF * `F` + bG * G,\n\t\ta ~ dnorm(0, 1),\n\t\tbF ~ dnorm(0, 1),\n\t\tbG ~ dnorm(0, 1),\n\t\tsigma ~ dexp(1)\n\t),\n\tdata = f2\n)\nrethinking::precis(m_6h5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               mean         sd       5.5%      94.5%\na      1.467513e-05 0.08752970 -0.1398747  0.1399040\nbF     5.602422e-01 0.19672784  0.2458332  0.8746513\nbG    -6.435327e-01 0.19672896 -0.9579436 -0.3291218\nsigma  9.463559e-01 0.06178686  0.8476086  1.0451032\n```\n\n\n:::\n:::\n\n\n\nOk, interestingly we can now see a strong positive effect of $F$ and a\nstrong negative effect of $G$. If we plot the data stratified by group size, we\ncan understand this effect a bit better. This is an example of a masked\nrelationship. Overall, average food appears to have a negative effect on\nweight, which doesn't seem to make sense.\n\nHowever, for groups of a given size, the more food there is, the heavier\nthose foxes tend to be. But for healthier (heavier) foxes, it is likely that\nmore new foxes are born, and despite the fact that the foxes are in a more\nabundant area, there is less food per fox. So within a group size, having\nmore food is good. But if the group size expands without a simultaneous\nincrease in food supply, there will be less food available for each fox.\nHowever, note that for the high group sizes, only one group was observed\nin each. So perhaps our estimates contain selection bias, if the groups that\nwere recorded are not typical examples of foxes with high group sizes.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(foxes, aes(x = avgfood, y = weight, col = factor(groupsize))) +\n\tgeom_point(alpha = 0.5) +\n\tgeom_smooth(method = \"lm\", alpha = 0.5) +\n\thgp::theme_ms() +\n\tgeom_smooth(method = \"lm\", aes(group = 1, color = \"overall\")) +\n\tscale_color_manual(\n\t\tvalues = c(viridisLite::viridis(7), \"black\")\n\t) +\n\tlabs(x = \"Average food\", y = \"Weight\", col = \"Group size\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](cp6_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\n\n## 6H6 and 6H7\n\nI decided to skip the open research questions.\n\n\n<!-- END OF FILE -->\n",
    "supporting": [
      "cp6_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}