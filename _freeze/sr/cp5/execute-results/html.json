{
  "hash": "c627640e5e95f984ff6ff52a1981f74d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 5: The Many Variables and the Spurious Waffles\"\ndate: 2022-06-22\n---\n\n\n\nThis chapter discusses the \"causal salad\" issue that is really prevelant in epidemiology (and other sciences) right now. When you \"adjust\" for variables in models, what are you actually doing? What answers can you get from adjusting? How do you decide what variables should go into a model? We get to talk about confounding, which is one of my favorite subjects, and more generally, other types of biases. These are presented in the framework of graphical causal models using directed acyclic graphs (DAGs).\n\n## Chapter notes\n\n* This chapter is mainly concerned with the issue of confounding, although it doesn't used the same technical terms that I learned in my epidemiology classes.\n* Specifically, we are concerned with **spurious associations** (positive confounding) where a third variable causes the relationship between two variables to appear stronger than it is;\n* And with **masked relationships** (negative confounding), where a third variable causes the relationship between two variables to appear weaker than it is.\n* This chapter also introduces **Directed Acyclic Graphs (DAGs)** as heuristic graphical causal models for understanding the causal relationships between variables.\n* The **conditional independencies** of DAGs are disccused, which are statements of which variables should be associated with each other in the data or not, given that the DAG is an accurate causal model. DAGs with the same variables and implied conditional independencies are called **Markov Equivalent**.\n* This chapter also discusses three ways to visualize the results of a regression model: predictor residual plots, posterior prediction plots, and counterfactual plots.  In this case, the counterfactual plot does not necessarily mean what I am used to it meaning, it just means we are predicting values which may not have been observed using the model (so in some sense they are counterfactual to our observed data).\n* Finally, this chapter also discusses categorical variables and index coding, which is used by the rethinking package (and later by Stan) rather than the dummy coding used by most R models.\n\n## Exercises\n\n### 5E1\n\nThe linear models\n\n$$\\mu_i = \\beta_x x_i + \\beta_z z_i$$\nand\n$$\\mu_i = \\alpha + \\beta_x x_i + \\beta_z z_i$$\n\nare both multiple regression models. The first model\n$$\\mu_i = \\alpha + \\beta x_i$$\nis a simple linear regression model and while the third model\n$$\\left( \\mu_i = \\alpha + \\beta (x_i - z_i) \\right)$$\ninvolves both $x$ and $z$, the model only has one coefficient and treats their difference as a single explanatory variable.\n\n### 5E2\n\nWe could evaluate the claim *animal diversity is linearly related to latitude, but only after controlling for plant diversity* using the linear model\n\\begin{align*}\n\\text{animal diversity}_i &\\sim \\mathrm{Likelihood}\\left( \\mu_i \\right) \\\\\n\\mu_i &= \\alpha + \\beta_1 \\left( \\mathrm{latitude} \\right) + \\beta_2 \\left( \\text{plant diversity} \\right)\n\\end{align*}\n\nwhere suitable priors are assigned and other appropriate parameters are given for the likelihood function.\n\n### 5E3\n\nWe could evaluate the claim *neither amount of funding nor size of laboratory is by itself a good predictor of time to PhD degree; but together these variables are both positively associated with time to degree* using the multiple linear regression\n\\begin{align*}\n\\text{time to PhD}_i &\\sim \\mathrm{Likelihood}\\left( \\mu_i \\right) \\\\\n\\mu_i &= \\alpha + \\beta_1 \\left( \\text{amount of funding} \\right) + \\beta_2 \\left( \\text{size of laboratory} \\right)\n\\end{align*}\n\nwith suitable priors, etc. The slope of both $\\beta_j$ should be positive. Classically, I would probably be inclined to include an interaction term in this model, but we haven't talked about that yet in the book so I didn't.\n\n### 5E4\n\nIf we have a single categorical variable with levels $A,$ $B,$ $C,$ and $D,$ (represented as indicator variables), the following linear models are inferentially equivalent:\n\\begin{align*}\n\\mu_i &= \\alpha + \\beta_A A_i + \\beta_B B_i + \\beta_D D_i, \\\\\n\\mu_i &= \\alpha + \\beta_B B_i + \\beta_C C_i + \\beta_D D_i, \\\\\n\\mu_i &= \\alpha_A A_i + \\alpha_B B_i + \\alpha_C C_i + \\alpha_D D_i, \\quad \\text{ and }\\\\\n\\mu_i &= \\alpha_A \\left( 1 - B_i - C_i - D_i \\right) + \\alpha_B B_i + \\alpha_C C_i + \\alpha_D D_i.\n\\end{align*}\n\n### 5M1\n\nAn example of a spurious correlation: I am happy on days when it is sunny outside, and when I get to leave work early. Both of these things individually make me happy, but the weather doesn't determine whether I get to leave work early. (At least not fully anyways. The weather definitely determines how much work I get done, but other external factors control the amount of work I have and the deadlines I need to meet.)\n\n### 5M2\n\nAn example of a masked relationship that I like: supposed you have multiple measurements of how far the accelerator is pressed in a car and the car's speed (taken simultaneously) at multiple time points, and you see no correlation. However, you are then given the measurements of the slope of the road at each of those time points, and you see that, when the slope of the road is taken into account, the two variables are correlated. When the car is going uphill, the accelerator is always pressed further and the speed is always lower, but pressing the accelerator still increases the speed.\n\n### 5M3\n\nI guess a higher divorce rate could cause a higher marriage rate by making more people available to be married. If divorced people tend to get remarried (potentially to non-divorced people), then the overall marriage rate could go up. Addressing this using a multiple linear regression model would be quite difficult, as you would need more data on remarriage and divorce status. It could be difficult to incorporate remarriages into the regression model, maybe an agent-based model would be more intuitive for this.\n\n### 5M4\n\nI found [this list](https://worldpopulationreview.com/state-rankings/mormon-population-by-state) of percent LDS population by state. So if we want to add this as a predictor to the divorce rate model, first I'll join these data to the WaffleDivorce data from the `rethinking` package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npct_lds <- readr::read_csv(here::here(\"static/pct_lds.csv\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 50 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): State\ndbl (3): mormonPop, mormonRate, Pop\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(rethinking)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: cmdstanr\nThis is cmdstanr version 0.8.1.9000\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n- CmdStan path: C:/Users/Zane/.cmdstan/cmdstan-2.34.1\n- CmdStan version: 2.34.1\n\nA newer version of CmdStan is available. See ?install_cmdstan() to install it.\nTo disable this check set option or environment variable cmdstanr_no_ver_check=TRUE.\nLoading required package: posterior\nThis is posterior version 1.6.0\n\nAttaching package: 'posterior'\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\nLoading required package: parallel\nrethinking (Version 2.40)\n\nAttaching package: 'rethinking'\n\nThe following object is masked from 'package:stats':\n\n    rstudent\n```\n\n\n:::\n\n```{.r .cell-code}\ndata(\"WaffleDivorce\")\n\ndat_5m4 <-\n\tdplyr::left_join(\n\t\tWaffleDivorce,\n\t\tpct_lds,\n\t\tby = c(\"Location\" = \"State\")\n\t)\n\ndat_5m4[\n\twhich(dat_5m4$Location == \"District of Columbia\"),\n\t\"mormonRate\"\n] <-0.0038\n\ndat_5m4_l <-\n\tdat_5m4 |>\n\tdplyr::transmute(\n\t\tD = Divorce,\n\t\tM = Marriage,\n\t\tA = MedianAgeMarriage,\n\t\tL = mormonRate * 100\n\t) |>\n\tas.list()\n\nlayout(matrix(c(1, 2, 3, 4), nrow = 2, ncol = 2))\npurrr::walk2(dat_5m4_l, names(dat_5m4_l), ~hist(.x, main = .y, breaks = \"FD\"))\n```\n\n::: {.cell-output-display}\n![](cp5_files/figure-html/5m4 data cleaning-1.png){width=672}\n:::\n\n```{.r .cell-code}\nlayout(1)\n```\n:::\n\n\n\nWhen I tried to do this the first time, it turns out that the WaffleDivorce dataset has D.C. in it, but not the state of Nevada? And the table of percent LDS populations I found has Nevada, but not D.C. So I just googled it, and on the Wikipedia page I saw that the percentage was 0.38% in 2014, which is good enough for government work, so I filled it in manually. I didn't transform any of the predictors, but standardizing and transforming them would probably be a good idea. I think a logit transformation would probably be suitable for the percent LDS population but as long as `quap()` converges I won't worry about it too much.\n\n\\begin{align*}\n\\text{Divorce rate}_i &\\sim \\mathrm{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\beta_0 + \\beta_1 \\cdot \\text{Marriage rate}_i + \\beta_2 \\cdot \\text{Median age at marriage}_i \\\\\n&\\quad\\quad + \\beta_3 \\cdot \\text{Percent LDS}_i \\\\\n\\beta_0 &\\sim \\mathrm{Unif}(0, 1000); \\\\\n\\beta_j &\\sim \\mathrm{Normal}(0, 10); \\quad j = \\{0, \\ldots, 3\\} \\\\\n\\sigma &\\sim \\mathrm{Exp}(0.5)\n\\end{align*}\n\nwhere $i$ indexes the states. Since the outcome is in units of divorces per 1000 people, I decided to let the intercept be anything from 0 to 1000 people. It will probably be quite small but that shouldn't be too much of a problem I think. Then, I assigned weakly uninformative priors to the slope coefficients and a simple positive prior to the standard deviation. It would be better to do a prior predictive simulation and figure out some better assumptions. Now we'll fit th model with `quap` (quadratic approximation). \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(100)\nfit_5m4 <-\n\trethinking::quap(\n\t\tflist = alist(\n\t\t\tD ~ dnorm(mu, sigma),\n\t\t\t# We could rewrite this so we didn't have to write out all the identical\n\t\t\t# priors but this is easier and I am lazy\n\t\t\tmu <- b0 + bM * M + bA * A + bL * L,\n\t\t\tb0 ~ dunif(0, 100),\n\t\t\tbM ~ dnorm(0, 10),\n\t\t\tbA ~ dnorm(0, 10),\n\t\t\tbL ~ dnorm(0, 10),\n\t\t\tsigma ~ dexp(0.5)\n\t\t),\n\t\tdata = dat_5m4_l\n\t)\n\nrethinking::precis(fit_5m4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              mean         sd       5.5%       94.5%\nb0    38.634820524 6.90452240 27.6000602 49.66958086\nbM     0.003608067 0.07550132 -0.1170576  0.12427376\nbA    -1.105494100 0.22397859 -1.4634551 -0.74753306\nbL    -0.066488235 0.02396230 -0.1047846 -0.02819184\nsigma  1.331562503 0.13193156  1.1207104  1.54241461\n```\n\n\n:::\n:::\n\n\n\nWe can see that for every one percentage increase in the LDS population of a state, the model predicts that the divorce rate will decrease by -0.07 units. Since the divorce rate is in percentage units, this means we would need slightly less than a 15% increase in LDS population for a state's divorce rate to decrease by 1%.\n\nThis estimate is probably biased by Utah, which is a strong outlier with 63% LDS population (far more than the second highest state, Idaho, with 24% of the population identifying as LDS).\n\n### 5M5\n\nFor this exercise, I'll call the price of gas $G$, obesity $O$, exercise $E$, and eating at restaurants $R$. The dag for this hypothesis looks like this.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndag <- dagitty::dagitty(\n\t\"dag {\n\t\tG -> E\n\t\tG -> R\n\t\tG -> O\n\t\tE -> O\n\t\tR -> O\n\t}\"\n)\n\nplot(dagitty::graphLayout(dag))\n```\n\n::: {.cell-output-display}\n![](cp5_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\nTherefore, $G$ confounds the relationship of $E$ on $O$ and also confounds the relationship of $R$ on $O$. (The direct causal effect of $G$ on $O$ represents the total impact of any other pathways that we have not measured.) We could capture both of these pathways simultaneously with the multiple regression\n$$\\mu_i = \\alpha + \\beta_E E_i + \\beta_R R_i + \\beta_G G_i,$$\nwhere $\\mu_i$ is the conditional mean of $O_i$. This model \"controls for\" the price of gas, and additionally controls for the effect of $E$ and $R$ on each other. If we are certain that $E$ and $R$ are related only though $G$, we could fit the two regression models\n$$\\mu_i = \\alpha + \\beta_E E_i + \\beta_G G_i$$\nand\n$$\\mu_i = \\alpha + \\beta_R R_i + \\beta_G G_i.$$\nUsing these two models, we control for the effect of gas price and obtain the direct causal effect of $E$ and $R$, assuming that $E$ and $R$ do not affect each other at all.\n\n### 5H1\n\nAssuming the DAG for the divorce problem is $M \\to A \\to D$. The DAG only has one conditional independency: $D$ and $M$ are uncorrelated if we condition on $A$. We can check this using dagitty as well.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndag2 <- dagitty::dagitty(\"dag {M -> A -> D}\")\ndagitty::impliedConditionalIndependencies(dag2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nD _||_ M | A\n```\n\n\n:::\n:::\n\n\n\nNow we can check if the data are consistent with this DAG. (We already know that it is, because this is the same conditional independency set as one of the previous example DAGs).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <-\n\tWaffleDivorce |>\n\tdplyr::select(\n\t\tD = Divorce,\n\t\tM = Marriage,\n\t\tA = MedianAgeMarriage\n\t) |>\n\tdplyr::mutate(dplyr::across(tidyselect::everything(), standardize))\n\n# Fit the model without age so we can see the unconditional estimate\nmodel_noage <-\n\t\trethinking::quap(\n\t\tflist = alist(\n\t\t\tD ~ dnorm(mu, sigma),\n\t\t\tmu <- a + bM * M,\n\t\t\ta ~ dnorm(0, 0.2),\n\t\t\tbM ~ dnorm(0, 0.5),\n\t\t\tsigma ~ dexp(1)\n\t\t),\n\t\tdata = d\n\t)\n\n# Fit the model with age only\nmodel_ageonly <-\n\trethinking::quap(\n\t\tflist = alist(\n\t\t\tD ~ dnorm(mu, sigma),\n\t\t\tmu <- a + bA * A,\n\t\t\ta ~ dnorm(0, 0.2),\n\t\t\tbA ~ dnorm(0, 0.5),\n\t\t\tsigma ~ dexp(1)\n\t\t),\n\t\tdata = d\n\t)\n\n# Fit the model with age to see the estimate after conditioning\nmodel_5h1 <-\n\trethinking::quap(\n\t\tflist = alist(\n\t\t\tD ~ dnorm(mu, sigma),\n\t\t\tmu <- a + bM * M + bA * A,\n\t\t\ta ~ dnorm(0, 0.2),\n\t\t\tbM ~ dnorm(0, 0.5),\n\t\t\tbA ~ dnorm(0, 0.5),\n\t\t\tsigma ~ dexp(1)\n\t\t),\n\t\tdata = d\n\t)\n\ncoeftab_plot(\n\tcoeftab(model_noage, model_ageonly, model_5h1),\n\tpar = c(\"bA\", \"bM\")\n)\n```\n\n::: {.cell-output-display}\n![](cp5_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\nWe can see that before we add age to the model, the effect of marriage is quit large. But then when we condition on age, the effect is close to zero with a large amount of uncertainty. So it appears that our data are consistent with the conditional independencies of the model. The coefficient for age does not change when we condition it on marriage rate, so this supports our conclusion.\n\n### 5H2\n\nAssuming that this is the true DAG for the divorce example, we want to fit a new model and estimate the counterfactual effect of halving a state's marriage rate $M$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(100)\nm_5h2 <-\n\trethinking::quap(\n\t\tflist = alist(\n\t\t\t## M -> A\n\t\t\tA ~ dnorm(mu_A, sigma_A),\n\t\t\tmu_A <- b0_A + bM * M,\n\t\t\tb0_A ~ dnorm(0, 0.2),\n\t\t\tbM ~ dnorm(0, 0.5),\n\t\t\tsigma_A ~ dexp(1),\n\t\t\t\n\t\t\t## A -> D\n\t\t\tD ~ dnorm(mu_D, sigma_D),\n\t\t\tmu_D <- b0_D + bA * A,\n\t\t\tb0_D ~ dnorm(0, 0.2),\n\t\t\tbA ~ dnorm(0, 0.5),\n\t\t\tsigma_D ~ dexp(1)\n\t\t),\n\t\tdata = d\n\t)\n\nrethinking::precis(m_5h2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 mean         sd       5.5%      94.5%\nb0_A    -6.922370e-08 0.08684788 -0.1387998  0.1387996\nbM      -6.947376e-01 0.09572699 -0.8477278 -0.5417474\nsigma_A  6.817373e-01 0.06758016  0.5737312  0.7897434\nb0_D    -4.353665e-07 0.09737877 -0.1556305  0.1556296\nbA      -5.684028e-01 0.10999981 -0.7442038 -0.3926019\nsigma_D  7.883257e-01 0.07801134  0.6636485  0.9130029\n```\n\n\n:::\n\n```{.r .cell-code}\n# Get the halved marriage rate for each state and standardize to model units\nwith(\n\tWaffleDivorce,\n\tM_seq <<- c(\n\t\t((Divorce / 2) - mean(Divorce)) / sd(Divorce),\n\t\td$D\n\t)\n)\n\nsim_dat <- data.frame(M = M_seq)\ns <- sim(m_5h2, data = sim_dat, vars = c(\"A\", \"D\"))\nres <-\n\ts |>\n\t# stack the matrix columns on top of each other into one vector\n\tlapply(as.vector) |>\n\ttibble::as_tibble() |>\n\tdplyr::mutate(\n\t\tM = rep(M_seq, each = nrow(s$A))\n\t)\n\t\nres_diff <-\n\ttibble::tibble(\n\t\tA = s$A[, 2] - s$A[, 1],\n\t\tD = s$D[, 2] - s$D[, 1]\n\t)\n\ntest <-\n\tlapply(s, \\(x) tibble::tibble(\n\t\tmean = colMeans(x[, 51:100] - x[, 1:50]),\n\t\tlwr = apply(x[, 51:100] - x[, 1:50], 2, rethinking::PI)[1, ],\n\t\tupr = apply(x[, 51:100] - x[, 1:50], 2, rethinking::PI)[2, ]\n\t))\n\ntest2 <- test$D\ntest2 <- cbind(test2, WaffleDivorce)\n\nlibrary(ggplot2)\nggplot(test2, aes(y = forcats::fct_reorder(Location, mean),\n\t\t\t\t\t\t\t\t\tx = mean, xmin = lwr, xmax = upr)) +\n\tgeom_pointrange() +\n\tscale_x_continuous(\n\t\tlabels = function(x) scales::label_number()((x * sd(WaffleDivorce$Divorce)))\n\t) +\n\tlabs(\n\t\tx = \"Counterfactual effect on divorce rate of halving marriage rate (mean, 89% CI)\",\n\t\ty = NULL\n\t) +\n\thgp::theme_ms()\n```\n\n::: {.cell-output-display}\n![](cp5_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\nFrom the precis, we can derive that for every 1 unit increase in the marriage rate, we expect $\\beta_M$ units of change in the median age of marriage, and thus $\\beta_A \\beta_M$ units of change in the divorce rate, which works out to approximately $(-0.69)(-0.57) = 0.3933$. So if the marriage rate increases by 1 standard deviation, we expect the divorce rate to increase by about 0.4 standard deviations.\n\nWe could also (much more easily, in fact, I just didn't think about it until after I did this the hard way) compute the counterfactual effect for a range of values, and then look up whatever we wanted. The divorce rate measurements in the original data are only measured to the nearest tenth, so we just need to simulate the counterfactual effect of every one-tenth unit change in divorce rate over the range of observed rates.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate the sequence\nM_seq <-\n\tseq(\n\t\tfrom = 4,\n\t\tto = 16,\n\t\tby = 0.01\n\t)\n# Restandardize it using the original values for model units\nM_seq <- (M_seq - mean(WaffleDivorce$Marriage)) /\n\tsd(WaffleDivorce$Marriage)\n\n# Do the simulation\nsim_dat <- data.frame(M = M_seq)\ns <- sim(m_5h2, data = sim_dat, vars = c(\"A\", \"D\"))\n\n# Clean up the results\nD_res <-\n\ttibble::tibble(\n\t\tM = round(M_seq * sd(WaffleDivorce$Marriage) + mean(WaffleDivorce$Marriage),\n\t\t\t\t\t\t\tdigits = 2),\n\t\tmean = colMeans(s$D),\n\t\tlwr = apply(s$D, 2, rethinking::PI)[1, ],\n\t\tupr = apply(s$D, 2, rethinking::PI)[2, ]\n\t) |>\n\tdplyr::mutate(\n\t\tdplyr::across(c(mean, lwr, upr), \\(x) x * sd(WaffleDivorce$Divorce) +\n\t\t\t\t\t\t\t\t\t\tmean(WaffleDivorce$Divorce) |> round(digits = 2))\n\t)\n\nmanipulated <-\n\tWaffleDivorce |>\n\tdplyr::transmute(\n\t\tLocation, Loc, M = Marriage / 2, orig = Marriage, out = Divorce\n\t) |>\n\tdplyr::left_join(D_res, by = \"M\")\n\n# Make the plot\nD_res |>\n\tggplot(aes(x = M, y = mean, ymin = lwr, ymax = upr)) +\n\tgeom_ribbon(fill = \"gray\") +\n\tgeom_line(size = 0.75) +\n\tgeom_point(\n\t\tdata = manipulated,\n\t\tfill = \"white\",\n\t\tcolor = \"black\",\n\t\tshape = 21,\n\t\tstroke = 1.5,\n\t\tsize = 3\n\t) +\n\tlabs(\n\t\tx = \"Manipulated marriage rate\",\n\t\ty = \"Counterfactual divorce rate\"\n\t) +\n\thgp::theme_ms()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](cp5_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# manipulated |>\n# \tdplyr::mutate(id = factor(dplyr::row_number())) |>\n# \tggplot() +\n# \tgeom_segment(\n# \t\taes(x = orig, xend = M, y = out, yend = mean, color = id),\n# \t\tshow.legend = FALSE,\n# \t\talpha = 0.5\n# \t) +\n# \tgeom_point(\n# \t\taes(x = M, y = mean, shape = \"Counterfactual\", fill = id),\n# \t\tsize = 3, color = \"black\"\n# \t) +\n# \tgeom_point(\n# \t\taes(x = orig, y = out, shape = \"Observed\", fill = id),\n# \t\tsize = 3, color = \"black\"\n# \t) +\n# \tguides(\n# \t\tfill = guide_none()\n# \t) +\n# \tscale_shape_manual(values = c(21, 22)) +\n# \tlabs(\n# \t\tx = \"Marriage rate\",\n# \t\ty = \"Divorce rate\"\n# \t) +\n# \thgp::theme_ms()\n```\n:::\n\n\n\nThere we go. The white points here show each of the states if their divorce rate were halved (and the model is true). I didn't label them because I had already spend too much time on this project. There's a lot more I could think of to do on this problem, but instead I decided to move on to the next one instead.\n\n### 5H3\n\nWe are given the following DAG for the milk energy problem.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndag3 <- dagitty::dagitty(\"dag {K <- M -> N -> K}\")\n```\n:::\n\n\n\nWe want to compute the counterfactual effect on K of doubling M, accounting for both the direct and indirect paths of causation.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"milk\")\nm <-\n\tmilk |>\n\tdplyr::transmute(\n\t\tN = standardize(neocortex.perc),\n\t\tM = standardize(log(mass)),\n\t\tK = standardize(kcal.per.g)\n\t) |>\n\ttidyr::drop_na()\n\nm_5h3 <-\n\trethinking::quap(\n\t\tflist = alist(\n\t\t\t# M -> N\n\t\t\tN ~ dnorm(mu_n, sigma_n),\n\t\t\tmu_n <- a_n + b_m * M,\n\t\t\ta_n ~ dnorm(0, 0.2),\n\t\t\tb_m ~ dnorm(0, 0.5),\n\t\t\tsigma_n ~ dexp(1),\n\t\t\t# M -> K <- N\n\t\t\tK ~ dnorm(mu_k, sigma_k),\n\t\t\tmu_k <- a_k + b_m * M + b_n * N,\n\t\t\ta_k ~ dnorm(0, 0.2),\n\t\t\tb_n ~ dnorm(0, 0.5),\n\t\t\tsigma_k ~ dexp(1)\n\t\t),\n\t\tdata = m\n\t)\n\nrethinking::precis(m_5h3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                mean        sd       5.5%     94.5%\na_n     -0.008405637 0.1274276 -0.2120595 0.1952482\nb_m      0.417649823 0.1672193  0.1504011 0.6848986\nsigma_n  0.681045996 0.1323715  0.4694908 0.8926012\na_k      0.026405034 0.1660113 -0.2389131 0.2917232\nb_n     -0.138437700 0.2789888 -0.5843156 0.3074402\nsigma_k  1.223352397 0.2214232  0.8694753 1.5772295\n```\n\n\n:::\n:::\n\n\n\nIn this case, the predictor we want the counterfactual effect of ($M$) is on a log scale, so we should be able to get the effect of halving it (since changes will be proportional on a multiplicative scale). But I'm not sure I formally understand what the \"total counterfactual effect\" is well enough to do that. So I'll simulate instead.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sequence in log units\nM_seq <- seq(from = -3, to = 5, by = 0.01)\n\n# Standardize with original values to model units\nM_seq <- (M_seq - mean(log(milk$mass), na.rm = TRUE)) /\n\tsd(log(milk$mass), na.rm = TRUE)\n\n# Simulate the predictions\nsim_dat <- data.frame(M = M_seq)\ns <- sim(m_5h3, data = sim_dat, vars = c(\"N\", \"K\"))\n\nplot_data <-\n\ttibble::tibble(\n\t\tM = exp(sim_dat$M * attr(m$M, \"scaled:scale\") +\n\t\t\tattr(m$M, \"scaled:center\")),\n\t\tK = colMeans(s$K) * attr(m$K, \"scaled:scale\") +\n\t\t\tattr(m$K, \"scaled:center\")\n\t)\nplot_PI <- apply(s$K, 2, PI) * attr(m$K, \"scaled:scale\") +\n\t\t\tattr(m$K, \"scaled:center\")\n\n# Plot the counterfactual effect\nplot(\n\tplot_data$M, plot_data$K,\n\ttype = \"l\",\n\txlab = \"manipulated mass (kg)\", ylab = \"counterfactual kilocalories per gram of milk\",\n\txlim = exp(c(-3, 5)),\n\tylim = c(0.1, 1.1)\n)\nshade(plot_PI, plot_data$M)\nmtext(\"Total counterfactual effect of M on K\")\n```\n\n::: {.cell-output-display}\n![](cp5_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\nAgain, the question didn't say *what* number to double, but you can get any of them from a simulation like this.\n\n### 5H4\n\nThis is an open-ended problem where we will consider how to add the indicator of being in the South to the marriage problem. There are a few possibilities for the causal implications of this.\n\n1. Something about Southerness directly affects age at marriage, marriage rate, and divorce rate all at the same time. (Or divorce rate, and one of the two predictors) That is, Southerness has both direct and indirect effects on divorce rate.\n1. Something about Souttherness directly affects age at marriage and marriage rate, having only indirect effects on divorce rate. Alternatively, Southerness only impacts one of these two variables.\n1. Something about Southernness directly affects divorce rate, with no indirect effects.\n\nMy first instinct is to say that age at marriage and marriage rate are primarily influenced by socioeconomic factors. Due to multiple historical factors (including slavery and an enduring legacy of systemic racism), the southern US, on average, has lower education rates and higher poverty rates as a region. Increased socioeconomic status tends to be associated with higher age of marriage, but a higher marriage rate and lower divorce rate (according to what I read while googling this).\n\nHowever, the southern U.S. also has a unique subculture (which varies widely across regions of the south), which could be a sociological cause of differences in some of these variables. For example, I think it is reasonable to say that traditional Southern culture encourages women to marry young, and also encourages women to get married in general -- there is definitely a stereotype about older, unmarried women in traditional Southern culture.\n\nSo, based on both the socioeconomic reasons and the \"culture\" argument, there are two models I would like to examine. First, the model that posits a direct effect of Southerness on divorce rate, as well as indirect effects on both age at marriage and marriage rate. Then, I'd also like to examine the model where there is no direct effect of Southerness on divorce rate, and Southerness acts on divorce rate via age at marriage and marriage rate.\n\nLet's examine the model with no direct effects first. We'll consider the conditional independencies of this DAG.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndag_indirect <- dagitty::dagitty(\"dag {M <- S -> A; D <- M -> A; A -> D}\")\nplot(dag_indirect)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nPlot coordinates for graph not supplied! Generating coordinates, see ?coordinates for how to set your own.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](cp5_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\ndagitty::impliedConditionalIndependencies(dag_indirect)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nD _||_ S | A, M\n```\n\n\n:::\n:::\n\n\n\nSo, if the data are consistent with this DAG, then the divorce rate should be independent of being in the South after we control for age at marriage and marriage rate. So let's fit a model that does that. Since Southern status is an indicator variable, I wasn't quite sure how to handle it in this model. In a frequentist framework, I would probably want to include an interaction term -- this was briefly mentioned at the beginning of this chapter, but has not been covered in detail so I'll just use a simple additive-effects-only model for now.\n\nBut first let's consider the conditional independencies of the DAG with direct effects.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndag_direct <- dagitty::dagitty(\"dag {M <- S -> A; D <- M -> A; A -> D; S -> D}\")\nplot(dag_direct)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nPlot coordinates for graph not supplied! Generating coordinates, see ?coordinates for how to set your own.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](cp5_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code}\ndagitty::impliedConditionalIndependencies(dag_direct)\n```\n:::\n\n\n\nThere are no conditional independencies for this graph! So we should expect to see a relationship for all three parameters in the model that conditions on all three of our predictor variables.\n\nFortunately for us, we can use the same model to evaluate both of these DAGs -- we just need to see which of the conditional independencies are supported by the result of predicting $D$ using all three of them.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <-\n\tWaffleDivorce |>\n\tdplyr::transmute(\n\t\tD = standardize(Divorce),\n\t\tM = standardize(Marriage),\n\t\tA = standardize(MedianAgeMarriage),\n\t\tS = South\n\t)\n\nm_s_only <-\n\t\trethinking::quap(\n\t\tflist = alist(\n\t\t\tD ~ dnorm(mu, sigma),\n\t\t\tmu <- a + bS * S,\n\t\t\ta ~ dnorm(0, 0.2),\n\t\t\tbS ~ dnorm(0, 0.5),\n\t\t\tsigma ~ dexp(1)\n\t\t),\n\t\tdata = d\n\t)\n\nm_m_only <-\n\t\trethinking::quap(\n\t\tflist = alist(\n\t\t\tD ~ dnorm(mu, sigma),\n\t\t\tmu <- a + bM * M,\n\t\t\ta ~ dnorm(0, 0.2),\n\t\t\tbM ~ dnorm(0, 0.5),\n\t\t\tsigma ~ dexp(1)\n\t\t),\n\t\tdata = d\n\t)\n\nm_a_only <-\n\t\trethinking::quap(\n\t\tflist = alist(\n\t\t\tD ~ dnorm(mu, sigma),\n\t\t\tmu <- a + bA * A,\n\t\t\ta ~ dnorm(0, 0.2),\n\t\t\tbA ~ dnorm(0, 0.5),\n\t\t\tsigma ~ dexp(1)\n\t\t),\n\t\tdata = d\n\t)\n\nm_all <-\n\trethinking::quap(\n\t\tflist = alist(\n\t\t\tD ~ dnorm(mu, sigma),\n\t\t\tmu <- a + bM * M + bA * A + bS * S,\n\t\t\ta ~ dnorm(0, 0.2),\n\t\t\tbM ~ dnorm(0, 0.5),\n\t\t\tbA ~ dnorm(0, 0.5),\n\t\t\tbS ~ dnorm(0, 0.5),\n\t\t\tsigma ~ dexp(1)\n\t\t),\n\t\tdata = d\n\t)\n\nrethinking::precis(m_all)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             mean         sd         5.5%      94.5%\na     -0.07590505 0.10600330 -0.245318792  0.0935087\nbM    -0.04224048 0.14778943 -0.278436533  0.1939556\nbA    -0.56163739 0.15090185 -0.802807694 -0.3204671\nbS     0.34998019 0.21572095  0.005216455  0.6947439\nsigma  0.76290395 0.07580295  0.641756198  0.8840517\n```\n\n\n:::\n\n```{.r .cell-code}\ncoeftab_plot(\n\tcoeftab(\n\t\tm_a_only, m_s_only, m_m_only, m_all\n\t),\n\tpars = c(\"bA\", \"bS\", \"bM\")\n)\n```\n\n::: {.cell-output-display}\n![](cp5_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\nSo, interestingly, from the estimated coefficients, we see that the coefficient for being in the South does not change that much when we control for both of the other variables. Of course, the CI crosses zero in the new model, so if we were doing bad statistics we would say that the effect has disappeared, but it was significant in the S-only model, which means that there is no direct effect of S. Since we are not doing bad statistics though, it seems unreasonable to claim that -- there appears to be an effect of both $A$ and $S$ in the final model.\n\nRecall our previous model which was consistent with the idea that marriage rate only impacts divorce rate through the effect of age of marriage. So another potential DAG is like this.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndag_med <- dagitty::dagitty(\"dag {M <- S -> A -> D; M -> A ->; S -> D}\")\nplot(dag_med)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nPlot coordinates for graph not supplied! Generating coordinates, see ?coordinates for how to set your own.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](cp5_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n```{.r .cell-code}\ndagitty::impliedConditionalIndependencies(dag_med)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nD _||_ M | A, S\n```\n\n\n:::\n:::\n\n\n\nSo the only casual independency of this DAG is whether D is independent of M after controlling for A and S, and after fitting our previous model we see that the data are consistent with this DAG. Arguably the data are not consistent with the other DAGs since those DAGs do not have any conditional independencies involving M, but the best model should still be chosen by a domain expert, not based on what the data are consistent with after observing the data.\n\n<!-- end of file -->\n",
    "supporting": [
      "cp5_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}