{
  "hash": "b47e3a957c4350b95f03c18d68395864",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"2023 Homework, week 2\"\ndate: 2023-01-27\n---\n\n\n\n\nThis homework covers the material from Lectures 3 and 4, and the content from\nbook Chapter 4. The questions are reproduced almost identically from\n[Richard McElreath's original assignment](https://github.com/rmcelreath/stat_rethinking_2023/blob/main/homework/week02.pdf), I did not write them. I only wrote these solutions.\n\n::: {.callout-note appearance=\"simple\" icon=false}\n\n**1.** From the Howell1 dataset, consider only the people younger than 13\nyears old. Estimate the causal association between age and weight. Assume that\nage influences weight through two paths. First, age influences height, and\nheight influences weight. Second, age directly influences weight through\nage-related changes in muscle growth and body proportions.\n\nDraw the DAG that represents these causal relationships. And then write a\ngenerative simulation that takes age as an input and simulates height\nand weight, obeying the relationships in the DAG.\n\n:::\n\nOK, I will assume that the first paragraph is just an introduction to the\nhomework set and the actual task for this question is in the second paragraph.\nHere is the DAG for this problem.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Specify the relationships in the DAG\ndag <-\n\tdagitty::dagitty(\n\t\t\"dag {\n\t\t\tage -> height -> weight\n\t\t\tage -> weight\n\t\t}\"\n\t)\n\n# Specify instructions for plotting the DAG, then do that\ndagitty::coordinates(dag) <-\n\tlist(\n\t\tx = c(age = 1, height = 2, weight = 2),\n\t\ty = c(age = 2, height = 3, weight = 1)\n\t)\nplot(dag)\n```\n\n::: {.cell-output-display}\n![](week2_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\n\nNow we can write a generative simulation. First, let's look at the pairwise\ncorrelations so we can get sort of an idea of the data distributions and the\neffects we should simulate.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rethinking)\ndata(Howell1)\nh1 <-\n\tHowell1 |>\n\tdplyr::filter(age < 13)\n\npairs(h1[1:3])\n```\n\n::: {.cell-output-display}\n![](week2_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n\nOk, so with that plot in mind we see that ages are discrete from 0 to 13,\nheight ranges from about 50 units to 150 units, and weight ranges from about\n5 units to 35 units. So our generative simulation should stay within those\nranges.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set the seed so the simulation makes the same numbers every time\nset.seed(101)\n\n# This part does the simulation and puts it into a tibble for storage\nsim <- tibble::tibble(\n\t# Just randomly draw an age. In the original data the ages are not\n\t# 100% even but I think this is fine.\n\tage = sample(0:13, nrow(h1), replace = TRUE),\n\t# Height and weight simulations using dag relationships and made up numbers.\n\theight = rnorm(nrow(h1), 60 + 5 * age, 10),\n\tweight = rnorm(nrow(h1), 3 + 0.1 * age + 0.2 * height, 1)\n)\n\n# Put the columns into the same order for easier comparisons and plot\nsim <- sim[, c(\"height\", \"weight\", \"age\")]\npairs(sim)\n```\n\n::: {.cell-output-display}\n![](week2_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\nI just randomly picked these numbers and fiddled with it a bit until the two\nplots looked similar, and I think I was able to get them pretty close for\nsuch a simple simulation using linear effects and normal errors.\n\n::: {.callout-note appearance=\"simple\" icon=false}\n\n**2.** Use a linear regression to estimate the **total** causal effect of\neach year of growth on weight.\n\n:::\n\nBased on the DAG, to obtain the **total** causal effect of a year of growth\n(the interpretation of the parameter associated with the independent\nvariable *age*) we want to use age as the only independent variable in the\nmodel. If we controlled for height, the parameter would estimate the\n**direct** causal effect of age, but we want the **total** effect. So the basic\nstructure of our model will look like this.\n\\begin{align*}\n\\text{weight}_i &\\sim \\text{Normal}(\\mu, \\sigma) \\\\\n\\mu &= \\alpha + \\beta \\cdot \\text{age}_i \\\\\n\\alpha &\\sim \\text{Prior}() \\\\\n\\beta &\\sim \\text{Prior}() \\\\\n\\sigma &\\sim \\text{Prior}()\n\\end{align*}\n\nWe will need to assign some priors to our data. In general, I tend to prefer\nweakly informative priors, whereas I think McElreath tends to prefer less\nbroad priors. I'll base my priors off the default recommended priors from\nthe [Stan devs' prior choice recommendations](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations).\nOf course they also recommend rescaling all variables before modeling, which\nI think is a good idea, but I won't do it here because I'm lazy and I don't\nthink it's going to be particularly useful here.\n\nOne additional constraint that we have is that the $\\alpha$ and $\\beta$\nparameters should both be **positive**!\nIt doesn't make sense for someone to shrink as they get older (at least not for\nages 0 to 13, maybe for seniors but not here). And it certainly doesn't make\nsense for someone to ever have a negative weight, even at age zero.\nSo we'll use a distribution that has to be positive. I'll choose a half-normal \ndistribution, which is easy to sample by just taking the absolute value of a\nrandom normal sample.\n\n\\begin{align*}\n\\text{weight}_i &\\sim \\text{Normal}(\\mu, \\sigma) \\\\\n\\mu &= \\alpha + \\beta \\cdot \\text{age}_i \\\\\n\\alpha &\\sim \\text{Half-Normal}(0, 1) \\\\\n\\beta &\\sim \\text{Half-Normal}(0, 1) \\\\\n\\sigma &\\sim \\text{Exponential}(1)\n\\end{align*}\n\nPerhaps we should do a prior predictive check\nto visualize them before doing anything else.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(101)\npps <-\n\ttibble::tibble(\n\t\ta = abs(rnorm(1000, 0, 1)),\n\t\tb = abs(rnorm(1000, 0, 1)),\n\t\ts = rexp(1000, 1)\n\t)\n\nplot(\n\tNULL,\n\txlim = c(0, 13), ylim = c(-10, 40),\n\txlab = \"Age\", ylab = \"Simulated mu\",\n\tmain = \"Prior predictive simulation of E[weight | age]\"\n)\n\nfor (i in 1:nrow(pps)) {\n\tcurve(\n\t\tpps$a[i] + pps$b[i] * x,\n\t\tfrom = 0, to = 13, n = 1000,\n\t\tadd = TRUE, col = rethinking::col.alpha(\"black\", 0.1)\n\t)\n}\n```\n\n::: {.cell-output-display}\n![](week2_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\nWell, some of those are way too flat, and some of them are way too steep, but\noverall I think this encompasses a good range of possibilities. Let's also\nlook at the prior predictive distribution of the actual outcomes. Here, I'll\ntake random samples of age from a discrete uniform distribution. That's\nprobably not the best way to do it but it seems easiest.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Do the simulation\nset.seed(102)\nsim_age <- sample(0:13, 1000, replace = TRUE)\nsim_y <- rnorm(\n\t1000,\n\tmean = pps$a + pps$b * sim_age,\n\tsd = pps$s\n)\nlayout(matrix(c(1, 2), nrow = 1))\n\n# Histogram of all y values\nhist(\n\tsim_y,\n\txlab = \"Simulated outcome\",\n\tmain = \"Distribution of simulated weights\",\n\tbreaks = \"FD\"\n)\n\n# Plot showing y vs x with simulated regression lines as well\nplot(\n\tNULL,\n\txlim = c(0, 13), ylim = c(-10, 40),\n\txlab = \"Age\", ylab = \"Simulated weight\",\n\tmain = \"Simulated weights vs ages\"\n)\nfor (i in 1:nrow(pps)) {\n\tcurve(\n\t\tpps$a[i] + pps$b[i] * x,\n\t\tfrom = 0, to = 13, n = 1000,\n\t\tadd = TRUE, col = rethinking::col.alpha(\"black\", 0.05)\n\t)\n}\npoints(sim_age, sim_y)\n```\n\n::: {.cell-output-display}\n![](week2_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n\nWell, we ended up with a few negative and a few ridiculously large weights,\nbut since we're just doing a linear regression here I think we can live with\nthat and let the data inform the golem that no one has negative heights.\nProbably we would want to either change the likelihood function or transform\nsomething (e.g. use a log link) to prevent any negative responses, but this\nwill probably wash out in the fitting. So let's do that.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <-\n\trethinking::quap(\n\t\tflist = alist(\n\t\t\tweight ~ dnorm(mu, sigma),\n\t\t\tmu <- a + b * age,\n\t\t\ta ~ dnorm(0, 1),\n\t\t\tb ~ dnorm(0, 1),\n\t\t\tsigma ~ dexp(1)\n\t\t),\n\t\tconstraints = alist(\n\t\t\ta = \"lower=0\",\n\t\t\tb = \"lower=0\"\n\t\t),\n\t\tdata = list(\n\t\t\tweight = h1$weight,\n\t\t\tage = h1$age\n\t\t)\n\t)\n```\n:::\n\n\n\n\nHmm. Looks like `quap` does not take a constraints argument the way I thought\nit did. So I guess we will just have to settle for exponential priors, which\nlike I mentioned, is probably the easiest way (not the best way) to get\na strictly positive prior. Let's redo the prior predictive simulation using\nthis model.\n\\begin{align*}\n\\text{weight}_i &\\sim \\text{Normal}(\\mu, \\sigma) \\\\\n\\mu &= \\alpha + \\beta \\cdot \\text{age}_i \\\\\n\\alpha &\\sim \\text{Exponential}(1) \\\\\n\\beta &\\sim \\text{Exponential}(1) \\\\\n\\sigma &\\sim \\text{Exponential}(1)\n\\end{align*}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlayout(matrix(c(1, 2), nrow = 1))\nset.seed(101)\npps <-\n\ttibble::tibble(\n\t\ta = rexp(1000, 1),\n\t\tb = rexp(1000, 1),\n\t\ts = rexp(1000, 1)\n\t)\n\nset.seed(102)\nsim_age <- sample(0:13, 1000, replace = TRUE)\nsim_y <- rnorm(\n\t1000,\n\tmean = pps$a + pps$b * sim_age,\n\tsd = pps$s\n)\n# Histogram of all y values\nhist(\n\tsim_y,\n\txlab = \"Simulated outcome\",\n\tmain = \"Distribution of simulated weights\",\n\tbreaks = \"FD\"\n)\n\n# Plot showing y vs x with simulated regression lines as well\nplot(\n\tNULL,\n\txlim = c(0, 13), ylim = c(-10, 150),\n\txlab = \"Age\", ylab = \"Simulated weight\",\n\tmain = \"Simulated weights vs ages\"\n)\nfor (i in 1:nrow(pps)) {\n\tcurve(\n\t\tpps$a[i] + pps$b[i] * x,\n\t\tfrom = 0, to = 13, n = 1000,\n\t\tadd = TRUE, col = rethinking::col.alpha(\"black\", 0.05)\n\t)\n}\npoints(sim_age, sim_y)\n```\n\n::: {.cell-output-display}\n![](week2_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n\nYes, this definitely results in a more left-skewed distribution, but I think\nthat is actually good, since we don't expect a lot of kids to weight 50+\nkilograms. So I'm not too pressed about\nit. Now let's finally fit the model, for real.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(101)\nfit <-\n\trethinking::quap(\n\t\tflist = alist(\n\t\t\tweight ~ dnorm(mu, sigma),\n\t\t\tmu <- a + b * age,\n\t\t\ta ~ dexp(1),\n\t\t\tb ~ dexp(1),\n\t\t\tsigma ~ dexp(1)\n\t\t),\n\t\tdata = list(\n\t\t\tweight = h1$weight,\n\t\t\tage = h1$age\n\t\t)\n\t)\n\nrethinking::precis(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          mean         sd     5.5%    94.5%\na     7.332870 0.35961024 6.758143 7.907596\nb     1.354753 0.05438219 1.267840 1.441667\nsigma 2.503612 0.14476058 2.272256 2.734967\n```\n\n\n:::\n:::\n\n\n\n\nSo our estimate for the total causal effect of age on weight is 1.35. In other words, we would expect that the average\nindividual is born at weight 7.33 units,\nand increases in weight by 1.35 units each\nyear.\n\n::: {.callout-note appearance=\"simple\" icon=false}\n\n**3.** Now suppose the causal association between age and weight might be\ndifferent for boys and girls. Use a single linear regression, with a\ncategorical variable for sex, to estimate the total causal effect of age on\nweight separately for boys and girls. How do girls and boys differ? Provide\none or more posterior contrasts as a summary.\n\n:::\n\nSo what we are assuming here is that the effect of age is different for\nmales and females -- we'll also allow the intercept to vary by sex, meaning that\nmales and females can also have a different weight at birth on average.\nWe'll use a similar model from before other than these changes.\n\\begin{align*}\n\\text{weight}_i &\\sim \\text{Normal}(\\mu, \\sigma) \\\\\n\\mu &= \\alpha_{\\text{Sex}_i} + \\beta \\cdot \\text{age}_{\\text{Sex}_i} \\\\\n\\alpha_{j} &\\sim \\text{Exponential}(1) \\\\\n\\beta_{j} &\\sim \\text{Exponential}(1) \\\\\n\\sigma &\\sim \\text{Exponential}(1)\n\\end{align*}\n\nSo now we will fit the model.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(103)\nfit2 <-\n\trethinking::quap(\n\t\tflist = alist(\n\t\t\tweight ~ dnorm(mu, sigma),\n\t\t\tmu <- a[sex] + b[sex] * age,\n\t\t\ta[sex] ~ dexp(1),\n\t\t\tb[sex] ~ dexp(1),\n\t\t\tsigma ~ dexp(1)\n\t\t),\n\t\tdata = list(\n\t\t\tweight = h1$weight,\n\t\t\tage = h1$age,\n\t\t\t# We have to add 1 for the index coding to work right\n\t\t\tsex = h1$male + 1\n\t\t),\n\t\tstart = list(a = c(1, 1), b = c(1, 1), sigma = 0.5)\n\t)\n\nrethinking::precis(fit2, depth = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          mean         sd     5.5%    94.5%\na[1]  6.861204 0.47573974 6.100880 7.621528\na[2]  7.664618 0.50386642 6.859343 8.469894\nb[1]  1.312828 0.07258895 1.196816 1.428839\nb[2]  1.414252 0.07545861 1.293655 1.534850\nsigma 2.406891 0.13948954 2.183960 2.629822\n```\n\n\n:::\n:::\n\n\n\n\nOk, so I had some issues with the start values here. Probably because\nthe priors are very diffuse, quap sometimes cannot get to the MAP from\nrandomly sampled starting locations. However, I just had to find a seed that\nworks, because it seems to be ignoring the start value for the vector-valued\nparameters (or I am specifying it incorrectly) in the error messages I get.\nBut this one fit, so let's go. There appears to be a slight difference between\nthe two slope parameters, but we cannot allow ourselves to be mislead by the\ntable of coefficients. We must compute the contrast distribution to truly\nunderstand what is happening here.\n\nNow, in order to understand the differences, we have to construct the contrast\ndistribution.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nN <- 2500\n\nage_vals <- 0:12\nmean_f <- rethinking::link(fit2, data = list(\"age\" = age_vals, \"sex\" = 1),\n\t\t\t\t\t\t\t\t\t\t\t\t\t n = N)\nmean_m <- rethinking::link(fit2, data = list(\"age\" = age_vals, \"sex\" = 2),\n\t\t\t\t\t\t\t\t\t\t\t\t\t n = N)\n\nmean_contrast <- mean_m - mean_f\n\nlayout(matrix(c(1, 2), nrow = 1))\nplot(\n\tNULL,\n\txlim = c(0, 12), ylim = c(0, 30),\n\txlab = \"Simulated age\",\n\tylab = \"Weight (kg)\"\n)\n\nfor (i in 1:N) {\n\tlines(\n\t\tage_vals, mean_m[i, ],\n\t\tlwd = 1, col = rethinking::col.alpha(\"dodgerblue2\", 0.01)\n\t)\n\tlines(\n\t\tage_vals, mean_f[i, ],\n\t\tlwd = 1, col = rethinking::col.alpha(\"hotpink1\", 0.01)\n\t)\n}\n\nlegend(\"topleft\", c(\"Males\", \"Females\"), col = c(\"dodgerblue2\", \"hotpink1\"),\n\t\t\t lty = c(1, 1))\n\nplot(\n\tNULL,\n\txlim = c(0, 12), ylim = c(-3, 5),\n\txlab = \"Simulated age\",\n\tylab = \"Weight contrast (M - F)\"\n)\n\nfor (i in 1:N) {\n\tlines(\n\t\tage_vals, mean_contrast[i, ],\n\t\tlwd = 1, col = rethinking::col.alpha(\"black\", 0.05)\n\t)\n}\n\nlines(age_vals, apply(mean_contrast, 2, mean), lwd = 3, lty = 2, col = \"red\")\nabline(h = 0, lwd = 3, lty = 3, col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](week2_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n\nHere, we can observe that the regression lines for males tend to be steeper\nthan the regression lines for females. From the contrast, we can see that\nmost of the time, the effect is positive and the line lies above zero.\nLet's compute a posterior interval for the mean.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlayout(1)\nplot(\n\tNULL,\n\txlim = c(0, 12),\n\tylim = c(-3, 5),\n\txlab = \"Simulated age\",\n\tylab = \"Weight contrast (M - F)\"\n)\n\nfor (p in c(seq(0.5, 0.9, 0.1), 0.95, 0.99)) {\n\tshade(apply(mean_contrast, 2, rethinking::PI, prob = p), age_vals)\n}\n\nlines(age_vals, apply(mean_contrast, 2, mean), lwd = 3, lty = 2)\nabline(h = 0, lwd = 3, lty = 3, col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](week2_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n\nFrom the contrast, we can see that men tend to be heavier at nearly all ages.\nThe weight of males and females tends to be closer at birth than on average\nat older ages.\n\nThis contrast is between the estimates of the conditional mean response. To\nincorporate individual variance into the uncertainty around the estimate,\nwe can also use samples from the posterior of the conditional mean, along with\nsamples from the posterior of the scale parameter to simulate individual\nresponses.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nN <- 2500\n\nage_vals <- 0:12\nmean_f <- rethinking::sim(fit2, data = list(\"age\" = age_vals, \"sex\" = 1),\n\t\t\t\t\t\t\t\t\t\t\t\t\t n = N)\nmean_m <- rethinking::sim(fit2, data = list(\"age\" = age_vals, \"sex\" = 2),\n\t\t\t\t\t\t\t\t\t\t\t\t\t n = N)\n\nmean_contrast <- mean_m - mean_f\n\nlayout(matrix(c(1, 2), nrow = 1))\nplot(\n\tNULL,\n\txlim = c(0, 12), ylim = c(-12, 12),\n\txlab = \"Simulated age\",\n\tylab = \"Weight contrast (M - F)\"\n)\n\nfor (i in 1:N) {\n\tlines(\n\t\tage_vals, mean_contrast[i, ],\n\t\tlwd = 1, col = rethinking::col.alpha(\"black\", 0.05)\n\t)\n}\n\nlines(age_vals, apply(mean_contrast, 2, mean), lwd = 3, lty = 2, col = \"red\")\nabline(h = 0, lwd = 3, lty = 3, col = \"blue\")\n\nplot(\n\tNULL,\n\txlim = c(0, 12),\n\tylim = c(-12, 12),\n\txlab = \"Simulated age\",\n\tylab = \"Weight contrast (M - F)\"\n)\n\nfor (p in c(seq(0.5, 0.9, 0.1), 0.95, 0.99)) {\n\tshade(apply(mean_contrast, 2, rethinking::PI, prob = p), age_vals)\n}\n\nlines(age_vals, apply(mean_contrast, 2, mean), lwd = 3, lty = 2)\nabline(h = 0, lwd = 3, lty = 3, col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](week2_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\n\nInterestingly, the variance tends to be quite large. However, as we see from\nthe plot on the left, we could probably use a better variance structure for\nthis model. If each line represents an individual, the spikes over time are\nunlikely to be this drastic, especially for children. We still see (from the\nright plot) that a given male is more likely to be heavier than a given female\nof the same age, but there is a lot of variation between individuals so this\nwill not always be true.\n\n\n::: {.callout-note appearance=\"simple\" icon=false}\n\n**4.** The data in `data(Oxboys)` are growth records for 26 boys measured\nover 9 periods. I want you to model their growth. Specifically, model the\nincrements in growth from one `Occassion` to the next. Each increment is simply\nthe difference between height in one occasion and height in the previous\noccasion. Since none of these boys shrunk during the study, all of the growth\nincrements are greater than zero. Estimate the posterior distribution of\nthese increments. Constrain the distribution so it is always positive -- it\nshould not be possible for the model to think that boys can shrink from year\nto year. Finally computer the posterior distribution of the total growth over\nall 9 occasions.\n\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(Oxboys)\ndplyr::glimpse(Oxboys)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 234\nColumns: 4\n$ Subject  <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3…\n$ age      <dbl> -1.0000, -0.7479, -0.4630, -0.1643, -0.0027, 0.2466, 0.5562, …\n$ height   <dbl> 140.5, 143.4, 144.8, 147.1, 147.7, 150.2, 151.7, 153.3, 155.8…\n$ Occasion <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3…\n```\n\n\n:::\n:::\n\n\n\n\nFrom the data documentation, we see that `age` and `Occasion` basically\nrepresent the same thing, so I'll ignore `age` for now. I think for this\nmodel, we don't even need to draw a DAG as there are only two variables we\nreally care about. \n\nThe first thing I'll do is the necessary data processing. We want to get the\nincrements in growth by subtracting the previous measurement from each height\nvalue. This will give us the amount of growth in between each measurement.\nNote that this will reduce us from 9 total measurements per boy to 8 total\ndifferences per boy, because there is nothing to subtract from the first\nmeasurement.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\no2 <- Oxboys |>\n\tdplyr::group_by(Subject) |>\n\tdplyr::mutate(prev = dplyr::lag(height)) |>\n\tdplyr::mutate(\n\t\tincrement = height - prev,\n\t\tperiod = Occasion - 1\n\t) |>\n\tdplyr::filter(\n\t\tOccasion != 1\n\t)\n\no2_l <- list(\n\tincrement = o2$increment,\n\tperiod = o2$period\n)\n```\n:::\n\n\n\n\nSo let's plot the distribution of each increment.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(100)\nlayout(cbind(\n\tmatrix(c(1:8), nrow = 2, ncol = 4, byrow = TRUE)\n))\n\n# Histograms of each increment\nfor (i in 1:8) {\n\tdat <- subset(o2, period == i)\n\thist(\n\t\tdat$increment,\n\t\tbreaks = seq(0, 5, 0.5),\n\t\txlab = NULL,\n\t\tmain = paste(\"Increment\", i),\n\t\tcex.lab = 1.25,\n\t\tcex.axis = 1.25\n\t)\n}\n```\n\n::: {.cell-output-display}\n![](week2_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n\n```{.r .cell-code}\nlayout(1)\n# Boxplot for each increment\nboxplot(\n\tincrement ~ period,\n\tdata = o2_l,\n\tcex.lab = 1.75,\n\tcex.axis = 1.75,\n\txlab = \"Period\", ylab = \"Increment\"\n)\nstripchart(\n\tincrement ~ period,\n\tdata = o2_l,\n\tmethod = \"jitter\",\n\tvertical = TRUE,\n\tadd = TRUE,\n\tpch = 21,\n\tcol = rethinking::col.alpha(\"red\", 0.5),\n\tcex = 1.25\n)\n```\n\n::: {.cell-output-display}\n![](week2_files/figure-html/unnamed-chunk-15-2.png){width=672}\n:::\n:::\n\n\n\n\nThere is definitely some variance in each increment, so I think it makes sense\nto use a model that looks like this\n\\begin{align*}\n\\text{Increment}_i &\\sim \\text{Lognormal}(\\mu, \\sigma) \\\\\n\\mu &= \\alpha_{\\text{Period}_i} \\\\\n\\alpha &\\sim \\text{Normal}(0, k) \\\\\n\\sigma &\\sim \\text{Exponential}(10)\n\\end{align*}\n\ninstead of a model that looks like this\n\n\\begin{align*}\n\\text{Increment}_i &\\sim \\text{Lognormal}(\\mu, \\sigma) \\\\\n\\mu &= \\alpha + \\beta_i \\cdot \\text{Period}_i \\\\\n\\alpha &\\sim \\text{Prior}() \\\\\n\\beta &\\sim \\text{Normal}(0, k) \\\\\n\\sigma &\\sim \\text{Exponential}(10)\n\\end{align*}\n\n> McElreath also chose to constrain the alpha parameters to all be the same,\nwhich I think is fine and makes the model much simpler. But since we were\ninterested in the posterior distribution of their sum anyways, I prefer\nto leave the model like this anyways. Besides, from the histogram we saw that\nsome of them were different -- I'm not sure if this is a good biological\nassumption or not though since I don't really know enough about this problem.\n\nwhere increment is the difference, and period is the same as occasion\nminus 1. We still need to choose $k$. Since we have a lognormal distribution,\nthe variance parameter should be biased to be small (untuitively we do this\nby making the exponential parameter larger) or else when we do the log\npart, the means will get really big. This is also true for $k$, which I decided\nto simulate instead of just picking to be big.\n\n> Note that I originally used a normal regression with a lognormal prior,\nbut I read McElreath's solution and decided to update this to use a\nlognormal regression with a normal prior, which I think is actually much\nbetter. If you use a Gaussian model for this, you run the risk of\nrandomly getting growth increments from the model that are negative, which\nwe specifically wanted to avoid in this problem.\n\n\nSo the increment over the first period is the difference between\nthe second height measurement and the first height measurement. (I also\nthink that this is what the problem is asking for.)\n\nIf we use period as an index variable, we could also include an \"overall\"\nintercept, but we don't really need to (the models are computationally\nequivalent parametrizations). Sometimes adding the overall intercept makes\nthe model run better, or so I've heard, but I'll ignore it for now.\n\nI choose a standard exponential prior on sigma, but for each alpha I choose\na lognormal prior. This also constrains the effect of each increment to\nbe strictly positive, but prefers a moderate effect rather than having\nmost of the weight near zero. In order to choose the variance constant $k$\nthat I wrote in above as a placeholder, I think it might be best to do a \nsmall prior predictive simulation. I'll just simulate one increment, since the\nmodel will allow them to be different regardless.\n\nI probably should have done this before looking at the data, but I think for\nnow it will be OK. We want to choose a diffuse prior, but not one that\nproduces insane results.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2323)\nk <- c(0.05, 0.1, 0.25, 0.5, 1, 1.5) # Values to try\nN <- 1000 # Number of simulations\n\n# Create a container for the output\npps <- matrix(nrow = N, ncol = length(k))\n\n# Do the simulation\nfor (i in 1:length(k)) {\n\tpps[, i] <- rlnorm(N, rnorm(N, 0, k[i]), rexp(N, 10))\n\t\n}\n\n# Make a plot\nlayout(matrix(1:length(k), nrow = 2, byrow = TRUE))\nfor (i in 1:length(k)) {\n\thist(pps[, i], breaks = \"FD\", main = paste(\"k =\", k[i]), xlab = NULL)\n} \n```\n\n::: {.cell-output-display}\n![](week2_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n\nWow, I forgot how hard it is to choose parameters for a lognormal prior. That\nscaling parameter really starts to act up quickly. So for this problem,\nI'll choose $k=0.5$ since it creates a prior that, in general, constrains our\nvalues to what seems \"reasonable\" but also allows the parameter to get\nlarge if it needs to, due to the tail of the distribution.\n\nSo to recap before we fit, our final model will be this.\n\\begin{align*}\n\\text{Increment}_i &\\sim \\text{Normal}(\\mu, \\sigma) \\\\\n\\mu &= \\alpha_{\\text{Period}_i} \\\\\n\\alpha &\\sim \\text{Lognormal}(0, 0.5) \\\\\n\\sigma &\\sim \\text{Exponential}(10)\n\\end{align*}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(105)\nfit_ob <-\n\trethinking::quap(\n\t\tflist = alist(\n\t\t\tincrement ~ dlnorm(mu, sigma),\n\t\t\tmu <- a[period],\n\t\t\ta[period] ~ dnorm(0, 0.5),\n\t\t\tsigma ~ dexp(10)\n\t\t),\n\t\tdata = o2_l,\n\t\tcontrol = list(maxit = 500)\n\t)\nrethinking::precis(fit_ob, depth = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            mean         sd        5.5%     94.5%\na[1]   0.3978845 0.10740207  0.22623527 0.5695338\na[2]   0.2322873 0.10739249  0.06065337 0.4039213\na[3]   0.5005341 0.10741057  0.32887130 0.6721970\na[4]  -0.1303368 0.10738912 -0.30196531 0.0412918\na[5]   0.1171602 0.10738882 -0.05446786 0.2887883\na[6]   0.6545573 0.10742686  0.48286841 0.8262461\na[7]   0.5295969 0.10741329  0.35792970 0.7012641\na[8]   0.3111911 0.10739646  0.13955079 0.4828314\nsigma  0.5606550 0.02697832  0.51753842 0.6037716\n```\n\n\n:::\n:::\n\n\n\n\nOkay, so we can see that all of the parameters are in the same sort of\nneighborhood, but they definitely appear to be somewhat different. However!\nRecall that we cannot allow ourselves to stare into the void and be mislead.\nWe shall first visualize the posterior distributions of these parameters.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1344134)\npost <- extract.samples(fit_ob)\nmn <- apply(post$a, 2, mean)\npi <- apply(post$a, 2, rethinking::PI)\n\nlayout(matrix(1:8, nrow = 2, byrow = TRUE))\nfor (i in 1:ncol(post$a)) {\n\t# Blank plot\n\tplot(\n\t\tNULL,\n\t\txlim = c(-1.5, 1.5), ylim = c(0, 4.5),\n\t\txlab = paste(\"Increment\", i, \"effect (cm)\"),\n\t\tylab = \"Density\",\n\t\txaxs = \"i\", yaxs = \"i\",\n\t\tcex.lab = 1.5,\n\t\tcex.axis = 1.5\n\t)\n\t\n\t# Mean and 89% PI\n\tabline(v = mn[i], col = \"red\", lty = 2, lwd = 2)\n\tabline(v = pi[1, i], col = \"red\", lty = 2)\n\tabline(v = pi[2, i], col = \"red\", lty = 2)\n\t\n\t# Density curve\n\trethinking::dens(post$a[, i], add = TRUE, lwd = 3)\n}\n\n# Common title\nmtext(\n\tpaste(\n\t\t\"Posterior distributions of individual increment effects\",\n\t\t\"(mean and 89% posterior interval)\",\n\t\tsep = \"\\n\"\n\t),\n\tside = 3,\n\tline = -3,\n\touter = TRUE,\n\tcex = 1.1\n\t)\n```\n\n::: {.cell-output-display}\n![](week2_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\n\nAgain, we must be careful not to make any undue comparisons here. These summaries\nare fine for thinking about individual effects, but it does not make sense\nto compare them across parameters.\nSo now we shall compute the contrast of interest. We can get the posterior\nof the total growth by creating a contrast across each of the growth\nincrements. Of course this contrast will look like\n$c^{\\mkern-1.5mu\\mathsf{T}} = \\langle 1, 1, \\ldots, 1 \\rangle$\nbecause we will just be adding up the samples.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlayout(1)\n\n# Sample the growth increments from the posterior\n# and then add them up\ncontrast <- sapply(\n\t1:1000,\n\t\\(i) sum(rlnorm(8, post$a[i, ], post$sigma[i]))\n)\ncontrast_mean <- mean(contrast)\n\nplot(\n\tNULL,\n\txlim = c(0, 30),\n\tylim = c(0, 0.18),\n\txlab = \"Sum of growth increments (cm)\",\n\tylab = \"Density\",\n\txaxs = \"i\", yaxs = \"i\",\n\tcex.axis = 1.25,\n\tcex.lab = 1.1,\n\ttcl = -0.25\n)\n\nplot_dens <- density(contrast, adjust = 0.5)\n\nfor (i in c(67, 89, 95, 99) / 100) {\n\tshade(plot_dens, PI(contrast, prob = i))\n}\n\nlines(\n\tx = rep(contrast_mean, 2),\n\ty = c(0, plot_dens$y[which.min(abs(plot_dens$x - contrast_mean))]),\n\tlty = 2, lwd = 2\n)\nrethinking::dens(contrast, lwd = 3, add = TRUE)\n```\n\n::: {.cell-output-display}\n![](week2_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n\n\nHere the dashed line shows the posterior mean. The dashed line shows the\nposterior mean, and the shaded regions show equal-tailed credible intervals.\nFrom darkest to lightest, the coverage for the CIs are, respectively: 67%,\n89%, 95%, 99%.\n\nI think we should also probably do this and include the individual-level\nvariance (the estimated sigma parameter), but for right now I'll call it a day.\nWe can see that the mean total growth was most likely to be 13 cm over the nine occasions, although a range of values from around 12 to 14 were also quite\nplausible.\n\n> This looks quite similar to McElreath's solution where the alpha for each\nincrement is required to be the same, with one major difference. The slope\nof the density is much more gradual to the left of the mean in my estimated\ndistribution. This could be sampling variation, or it could be that one (or more) of the\nincrements is typically smaller than the others in this population, and drags\ndown the overall average parameter value when they are constrained to be the same.\n\n<!-- END OF FILE -->\n",
    "supporting": [
      "week2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}